takentaal-amendment v1.0



# LLM2FPGA



LLM2FPGA aims to enable local inference of open-source Large Language Models (LLMs) on FPGAs using a fully open-source toolchain. While LLM inference has been demonstrated on proprietary hardware and software, we are not aware of any widely recognized project running open-source LLMs on FPGAs through a fully open-source EDA (Electronics Design Automation) flow. To fill this gap, the project will produce an HDL implementation of a lightweight open-source LLM, verify it via simulation, and then attempt synthesis and place-and-route on freely supported FPGA devices. By providing a fully open alternative to proprietary and cloud-based LLM inference, LLM2FPGA will offer a transparent, flexible, and privacy-friendly way to run your own LLM on local hardware.



## Survey & candidate selection



State of the art review. Although some candidate projects have been identified, to have the big picture of what has been achieved so far, we need to understand 10 to 15 papers/projects, and what can be reused, even finding synergies between papers/projects.



We are lucky that the keywords LLM and FPGA usually give highly relevant results.



The first subtask is desk research. Survey table of 10+ FPGA-LLM papers/repos, answering the following questions:



Openness: Is the code or detailed design public or promised? Open to open-source FPGA tools? Uses open LLMs?



Hardware: Avoid if it needs extra hardware. Check FPGA and open toolchain support.



Design: Note method (RTL/HLS), model size (<10M preferred), proof-of-concept potential, and proprietary tool dependencies.



Second subtask:



The purpose is to identify issues or blockages, not to get each project completely reproduced.



— Can the HDL (SystemVerilog, Verilog) be parsed by Yosys or yosys-slang?



— If HLS is used, can the original C/C++ code be compiled with Vericert or another FOSS HLS tool?



Issues or blockages such as:



— Uses features of HDL (SystemVerilog, VHDL) that are not supported in yosys.



In yosys, Verilog support is more mature than SystemVerilog (with yosys-slang) or VHDL (with ghdl-yosys-plugin)



— Same for HLS



The third subtask:



— Chosen kernel + why



— Any major blockers found in other candidates



— Fallback plan in case current kernel becomes unviable



From now on, we call the chosen kernel the *selected route*.



* Publish survey results based on desk research.

* Research compatibility of potential projects with open source tooling: create a script that clones each repo and runs elaboration with Yosys; logs archived (green, blockages etc)

* Write up and publish results of the research and next steps. (chosen kernel and why + fallback)



## End-to-End semantic equivalence of a PyTorch kernel to synthesizable RTL

Goal: Demonstrate that the selected pipeline can lower a minimal PyTorch kernel to synthesizable RTL, integrate it into a complete FPGA top-level, generate a bitstream, and run a fixed test vector on real hardware with outputs matching the PyTorch reference.

Risks and mitigations:
  PyTorch hard to simulate: it seems well documented https://docs.pytorch.org/executorch/stable/getting-started.html
  Discrepancy between PyTorch and RTL simulation: simulate each MLIR stage with original PyTorch, to isolate the pass that creates discrepancy

Subtask 1 consists of:

flake.nix with all relevant tooling: torch-MLIR, LLVM MLIR, CIRCT, yosys.

Nix devshell provides all tools used by the pipeline.

Cached derivations for each pipeline stage, so reruns do not recompute earlier stages.

Also, for each upstream tool, support a build with debug symbols. Needed for debugging issues with tooling.

Subtask 2 consists of:
With the same PyTorch module used in 1.c, run a PyTorch simulation with fixed inputs. Run pipeline with that model, simulate resulting SystemVerilog and compare with golden reference.
Subtask 3 consists of:
  Yosys ingestion + RTLIL netlist gate

Artifact: matmul.il produced by Yosys from matmul.sv.

Yosys completes elaboration and emits RTLIL without fatal errors.

Subtask 4 consists of:

Glue RTL for interface, Bitstream generation and testing
Think of the simplest way to interface with the host computer, generate bitstream of glue RTL + model RTL, test results are equivalent to golden reference.

Read and reuse previous work with this board:
https://www.cnblogs.com/ruidongwu/p/18564807

https://x.com/enjoy_digital/status/1924910401176719375

https://www.controlpaths.com/2025/05/18/kintex7-accelerator/
Subtask 5 consists of:
Report and demo of results

- nix flake and matmul.sv from executing it

- PyTorch simulation trace, systemverilog testbench and their comparison

- matmul.il and resource usage report with "yosys stat"

- FPGA integration + bring-up
  
- Write up of results, and video demo if succesful

## Lowering of small LLM to RTL

Goal: Test if a small open-source language model (TinyStories-1M) can be lowered from PyTorch to synthesizable RTL using a fully open-source compilation flow.

In this task we move from small matmul module to smallest available LLM.
This task establishes existence of a valid lowering path only; scalability and hardware execution are out of scope, for later tasks.

The succesful result is an unintegrated RTL netlist of the model (RTLIL or Verilog/SystemVerilog) and synthesis resource estimates, without stubbing operators.

Otherwise, bottleneck report identifying unsupported operators, failing passes, or any other error in the toolchain.

Functional equivalence to PyTorch is not evaluated in this task; only structural synthesizability is established. Semantic validation is addressed in the subsequent hardware validation task.

Risks and mitigations
Risks:

Unsupported MLIR operations found during lowering.

Invalid, non-synthesizable, or structurally incorrect RTL emitted by CIRCT.

Toolchain pass interactions leading to verifier failures or fatal errors at stage boundaries.

Mitigations:
Use -verify-each for each MLIR stage.

For failures, isolate minimal reproducing cases using mlir-reduce and archive them.

If necessary, adjust pass ordering. Final success must not rely on operator stubbing or blackboxes.

MLIR debugging docs:
https://mlir.llvm.org/getting_started/Debugging/

Subtask 1 consists of:
Frozen TinyStories-1M artifact
Subtask 2 consists of:
tiny_stories_1m.mlir, just before consumption by CIRCT
If a blocker exists, a reduced reproducer (mlir-reduce or minimized export input) is committed or archived.
The MLIR verifier passes at this boundary (-verify-each clean at this stage).
Subtask 3 consists of:
tiny_stories_1m.sv, generated by CIRCT
Generated RTL is structurally sanity-checked (at minimum: parsable by Yosys front-end in the pinned environment).
If a blocker exists, a reduced reproducer (mlir-reduce or minimized export input) is committed or archived.
Subtask 4 consists of:
  Artifact: tinystories_1m.il produced by Yosys from tinystories_1m.sv.
  Yosys completes elaboration and emits RTLIL without fatal errors.
Subtask 5 consists of:
  Artifact: resource utilization report (Yosys stat) generated from the .il/design.

Bottleneck report: unsupported operators encountered, failing passes, required stubs/workarounds, and/or any other error. Includes minimized reproducers.

  - TinyStories-1M PyTorch model
  - tiny_stories_1m.mlir, just before consumption by CIRCT
  - tiny_stories_1m.sv, generated by CIRCT
  - tiny_stories_1m.il, generated by yosys
  - synthesis resource report + bottleneck report

## FPGA integration and hardware validation

Goal: 
Demonstrate FPGA integration of the lowered TinyStories-1M model RTL on the YPCB-00338-1P1 board.

This includes:
- Integrating the generated RTL with minimal glue logic to interface with host computer,
- Completing full FPGA synthesis and place-and-route,
- Loading the design on hardware and executing a fixed input,
- Verifying hardware outputs against the PyTorch reference.

Risks and mitigations

Risks:

- Auto-generated interfaces may be impractical for real hardware integration.
- Errors are difficult to localize across lowering, synthesis, and hardware stages.

Mitigations:

- Base the host-facing interface and integration approach on existing, documented FPGA designs for the same board.

  Some references:
  https://www.cnblogs.com/ruidongwu/p/18564807

https://x.com/enjoy_digital/status/1924910401176719375

https://www.controlpaths.com/2025/05/18/kintex7-accelerator/

https://github.com/litex-hub/litex-boards/commit/6d58ae6b31d80b255de12c2d3f5bfefda4c38b90
- Treat the PyTorch model as the single source of truth and validate each stage against it.

Subtask 1 consists of:
Define and freeze the host-facing interface.

Use previous work extensively:
  https://www.cnblogs.com/ruidongwu/p/18564807

https://x.com/enjoy_digital/status/1924910401176719375

https://www.controlpaths.com/2025/05/18/kintex7-accelerator/

https://github.com/litex-hub/litex-boards/commit/6d58ae6b31d80b255de12c2d3f5bfefda4c38b90

Subtask 2 consists of:
Top-level RTL integration (model + glue)
Produce a single synthesizable top-level design.
Subtask 3 consists of:
FPGA synthesis and place-and-route, and usage report.
Subtask 4 consists of:
Board bring-up and execution

Demonstrate that the design runs on real hardware.
Subtask 5 consists of:
Hardware vs PyTorch equivalence check

Define fixed TinyStories-1M input.

Generate PyTorch reference output.

Compare FPGA output to reference (exact or tolerance-based).

- Interface specification document.
- Top-level SystemVerilog.
- Bitstream or PnR report.
- Programming logs.
- Reference output, Comparison report, Short demonstration write-up.