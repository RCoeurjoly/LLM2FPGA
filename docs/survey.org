
| Paper/Repo Name                                                                           | Open Source Code                                               | Open LLM?               | Extra HW?                           | FPGA Used                                | #Params   | LUT's  | Open Toolchain? | Design Style (RTL/HLS/etc)           | Tiny Model? | Proprietary Tools? | Notes                                                                                                                                |   |
|-------------------------------------------------------------------------------------------+----------------------------------------------------------------+-------------------------+-------------------------------------+------------------------------------------+-----------+--------+-----------------+--------------------------------------+-------------+--------------------+--------------------------------------------------------------------------------------------------------------------------------------+---|
| TeLLMe (https://arxiv.org/abs/2504.16266)                                                 | No                                                             | BitNet 1.58             | No, Runs entirely on AMD KV260 FPGA | AMD KV260 (Zynq UltraScale+ XCK26 MPSoC) | 0.7B      | 108994 | No              | HLS (Vitis)                          | No          | Yes                | Full end-to-end support (prefill + decode); energy-efficient; 9.51 tok/s                                                             |   |
| FPGA-Based Tiled MatMul on KV260 ([arXiv:2503.16731v3](https://arxiv.org/abs/2503.16731)) | Yes ([GitHub](https://github.com/Richielee630/TMMA))           | DistilBERT              | No                                  | AMD KV260 (Zynq UltraScale+ XCK26 MPSoC) | ~66M      | 71,050 | No              | HLS (Vitis)                          | Yes         | Yes                | Accelerates QKV GEMM ops only; HLS in C++; integrated into quantized DistilBERT with PYNQ                                            |   |
| TerEffic (https://arxiv.org/abs/2502.16473)                                               | No                                                             | MatMul-Free LM          | Yes (multi-FPGA or HBM)             | AMD U280 (16nm, 42MB SRAM, 8GB HBM)      | 370M–2.7B | 781k   | No              | RTL (Vivado)                         | No          | Yes                | Two variants: fully on-chip (multi-FPGA) and HBM-assisted; ternary quantized; very efficient                                         |   |
| MEADOW (https://arxiv.org/abs/2503.11663)                                                 | No                                                             | OPT-125M                | No                                  | Xilinx ZCU102 (UltraScale+)              | 125M      | 150k   | No              | RTL (Vivado + pipeline)              | Yes         | Yes                | Uses TPHS dataflow + weight packing; evaluated on OPT-125M and OPT-1.3B; 40% end-to-end latency gain                                 |   |
| FlightLLM (https://arxiv.org/abs/2401.03868)                                              | No (artifact: https://zenodo.org/doi/10.5281/zenodo.10422477)  | LLaMA2-7B, OPT-6.7B     | Yes (HBM or hybrid HBM+DDR)         | Xilinx Alveo U280; Versal VHK158         | 6.7B–7B   | 574k+  | No              | RTL (Vivado), full flow              | No          | Yes                | Full end-to-end inference; sparse DSP chains; on-chip decode; adaptive compilation; 6× energy efficiency vs V100S.                   |   |
| SECDA-LLM (https://arxiv.org/abs/2408.00462)                                              | No                                                             | TinyLlama               | No                                  | PYNQ-Z1 (Xilinx Zynq Z020)               | 1.1B      | ?      | No              | HLS (SystemC via SECDA)              | Yes         | Yes                | Custom MatMul accelerator using BFP quantization; 11× speedup vs ARM NEON CPU; tightly integrated with llama.cpp framework.          |   |
| MatMul-free LM (https://arxiv.org/abs/2406.02528)                                         | Yes (https://github.com/ridgerchu/matmulfreellm)               | Custom (MatMul-free LM) | No (runs at 13W, no HBM)            | Intel Stratix 10 (D5005 PAC)             | 370M–2.7B | ?      | No              | RTL (SystemVerilog)                  | No          | Yes                | Fully MatMul-free (ternary + elementwise ops); FPGA runs at 60 MHz; 62 tok/s @370M; ~10× lower memory use; custom assembler + ISA    |   |
| LoopLynx (https://arxiv.org/abs/2504.09561)                                               | Yes (https://github.com/zjnyly/LoopLynx)                       | GPT-2                   | Yes (multi-FPGA with HBM)           | AMD Alveo U50 (x2), U280                 | 345M      | 624K   | No              | HLS (Vitis), hybrid spatial-temporal | Yes         | Yes                | Hybrid spatial-temporal; 2.52× speedup vs A100 in decode; 4-node setup: 392 tok/s; scalable; task-level and intra-kernel pipelining. |   |
| MASE https://arxiv.org/abs/2307.15517v2                                                   | No                                                             | GPT-2                   | No                                  | Xilinx Alveo U250                        | 345M      | ?      | No              | RTL (SystemVerilog)                  | Yes         | Yes                | Focuses on speculative execution for decode stage; achieves 2× throughput vs baseline; supports KV cache and GPT-style LLMs.         |   |
| On-Device Qwen2.5 https://arxiv.org/abs/2504.17376                                        | No                                                             | Qwen2.5-0.5B            | No                                  | AMD KV260 (Zynq UltraScale+ XCK26 MPSoC) | 0.5B      | 96,553 | No              | HLS (Vitis), pipelined MAC           | Yes         | Yes                | Uses AWQ (INT4) with hybrid CPU+FPGA execution; 5.1 tok/s; 55.1% model compression rate                                              |   |
| HLSTransform https://arxiv.org/abs/2405.00738                                             | Yes (https://github.com/HLSTransform/submission)               | LLaMA 2                 | No                                  | Xilinx Virtex UltraScale+ VU9P           | 110M      | ?      | No              | HLS (Vitis)                          | Yes         | Yes                | Quantized (INT8, Q8_0); achieves 57 tok/s; 12.75× less energy/token vs CPU; open-source end-to-end flow.                             |   |
| Spatial acceleration https://arxiv.org/abs/2312.15159                                     | Yes (https://github.com/cornell-zhang/allo/tree/main/examples) | BERT, GPT-2             | No                                  | AMD Alveo U280                           | 110M–355M | 1.3M   | No              | HLS (Vitis), spatial dataflow        | Yes         | Yes                | Analytical model + open HLS kernels; FPGA outperforms A100 in GPT2 decode; 13.4× faster than prior BERT accelerators on FPGA.        |   |
| AccLLM https://arxiv.org/abs/2505.03745                                                   | No                                                             | LLaMA-7B, Falcon-7B     | Yes (multi-FPGA setup)              | AMD Alveo U280 (×4), Xilinx VCU118       | 7B        | 3.4M   | No              | RTL (Verilog/SystemVerilog)          | No          | Yes                | End-to-end deployment; hybrid offloading; sparse matmul + custom interconnect; 2.6× faster than V100 at 3.2× lower energy.           |   |
