
| Paper/Repo Name                                                                           | Open Source Code                                              | Open LLM?               | Extra HW?                           | FPGA Used                                | #Params   | LUT's  | Open Toolchain? | Design Style (RTL/HLS/etc)           | #Params | Tiny Model? | Proprietary Tools? | Notes                                                                                                                                |
|-------------------------------------------------------------------------------------------+---------------------------------------------------------------+-------------------------+-------------------------------------+------------------------------------------+-----------+--------+-----------------+--------------------------------------+---------+-------------+--------------------+--------------------------------------------------------------------------------------------------------------------------------------|
| TeLLMe (https://arxiv.org/abs/2504.16266)                                                 | No                                                            | BitNet 1.58             | No, Runs entirely on AMD KV260 FPGA | AMD KV260 (Zynq UltraScale+ XCK26 MPSoC) | 0.7B      | 108994 | No              | HLS (Vitis)                          | 0.7B    | No          | Yes                | Full end-to-end support (prefill + decode); energy-efficient; 9.51 tok/s                                                             |
| FPGA-Based Tiled MatMul on KV260 ([arXiv:2503.16731v3](https://arxiv.org/abs/2503.16731)) | Yes ([GitHub](https://github.com/Richielee630/TMMA))          | DistilBERT              | No                                  | AMD KV260 (Zynq UltraScale+ XCK26 MPSoC) | ~66M      | 71,050 | No              | HLS (Vitis)                          | 66M     | Yes         | Yes                | Accelerates QKV GEMM ops only; HLS in C++; integrated into quantized DistilBERT with PYNQ                                            |
| TerEffic (https://arxiv.org/abs/2502.16473)                                               | No                                                            | MatMul-Free LM          | Yes (multi-FPGA or HBM)             | AMD U280 (16nm, 42MB SRAM, 8GB HBM)      | 370M–2.7B | 781k   | No              | RTL (Vivado)                         | 370M+   | No          | Yes                | Two variants: fully on-chip (multi-FPGA) and HBM-assisted; ternary quantized; very efficient                                         |
| MEADOW (https://arxiv.org/abs/2503.11663)                                                 | No                                                            | OPT-125M                | No                                  | Xilinx ZCU102 (UltraScale+)              | 125M      | 150k   | No              | RTL (Vivado + pipeline)              | 125M    | Yes         | Yes                | Uses TPHS dataflow + weight packing; evaluated on OPT-125M and OPT-1.3B; 40% end-to-end latency gain                                 |
| FlightLLM (https://arxiv.org/abs/2401.03868)                                              | No (artifact: https://zenodo.org/doi/10.5281/zenodo.10422477) | LLaMA2-7B, OPT-6.7B     | Yes (HBM or hybrid HBM+DDR)         | Xilinx Alveo U280; Versal VHK158         | 6.7B–7B   | 574k+  | No              | RTL (Vivado), full flow              | 6.7B–7B | No          | Yes                | Full end-to-end inference; sparse DSP chains; on-chip decode; adaptive compilation; 6× energy efficiency vs V100S.                   |
| SECDA-LLM (https://arxiv.org/abs/2408.00462)                                              | No                                                            | TinyLlama               | No                                  | PYNQ-Z1 (Xilinx Zynq Z020)               | 1.1B      | ?      | No              | HLS (SystemC via SECDA)              | 1.1B    | Yes         | Yes                | Custom MatMul accelerator using BFP quantization; 11× speedup vs ARM NEON CPU; tightly integrated with llama.cpp framework.          |
| MatMul-free LM (https://arxiv.org/abs/2406.02528)                                         | Yes (https://github.com/ridgerchu/matmulfreellm)              | Custom (MatMul-free LM) | No (runs at 13W, no HBM)            | Intel Stratix 10 (D5005 PAC)             | 370M–2.7B | ?      | No              | RTL (SystemVerilog)                  | 2.7B    | No          | Yes                | Fully MatMul-free (ternary + elementwise ops); FPGA runs at 60 MHz; 62 tok/s @370M; ~10× lower memory use; custom assembler + ISA    |
| LoopLynx (https://arxiv.org/abs/2504.09561)                                               | Yes (https://github.com/zjnyly/LoopLynx)                      | GPT-2                   | Yes (multi-FPGA with HBM)           | AMD Alveo U50 (x2), U280                 | 345M      | 624K   | No              | HLS (Vitis), hybrid spatial-temporal | 345M    | Yes         | Yes                | Hybrid spatial-temporal; 2.52× speedup vs A100 in decode; 4-node setup: 392 tok/s; scalable; task-level and intra-kernel pipelining. |
| MASE https://arxiv.org/abs/2307.15517v2                                                   | No                                                            | GPT-2                   | No                                  | Xilinx Alveo U250                        | 345M      | ?      | No              | RTL (SystemVerilog)                  | 345M    | Yes         | Yes                | Focuses on speculative execution for decode stage; achieves 2× throughput vs baseline; supports KV cache and GPT-style LLMs.         |
| On-Device Qwen2.5 https://arxiv.org/abs/2504.17376                                        |                                                               |                         |                                     |                                          |           |        |                 |                                      |         |             |                    |                                                                                                                                      |
| HLSTransform https://arxiv.org/abs/2405.00738                                             |                                                               |                         |                                     |                                          |           |        |                 |                                      |         |             |                    |                                                                                                                                      |
| https://arxiv.org/abs/2312.15159                                                          |                                                               |                         |                                     |                                          |           |        |                 |                                      |         |             |                    |                                                                                                                                      |
| AccLLM https://arxiv.org/abs/2505.03745                                                   |                                                               |                         |                                     |                                          |           |        |                 |                                      |         |             |                    |                                                                                                                                      |
