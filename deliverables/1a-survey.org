
| Id | Phase | Paper/Repo Name                                                                           | Open Source Code                                                               | Open LLM?               | Extra HW?                           | FPGA Used                                | #Params   | LUT's  | Open Toolchain? | Design Style (RTL/HLS/etc)           | Tiny Model? | Proprietary Tools? | Notes                                                                                                                                |
|----+-------+-------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-------------------------+-------------------------------------+------------------------------------------+-----------+--------+-----------------+--------------------------------------+-------------+--------------------+--------------------------------------------------------------------------------------------------------------------------------------|
|  1 | 1a    | TeLLMe (https://arxiv.org/abs/2504.16266)                                                 | No                                                                             | BitNet 1.58             | No, Runs entirely on AMD KV260 FPGA | AMD KV260 (Zynq UltraScale+ XCK26 MPSoC) | 0.7B      | 108994 | No              | HLS (Vitis)                          | No          | Yes                | Full end-to-end support (prefill + decode); energy-efficient; 9.51 tok/s                                                             |
|  2 | 1a    | FPGA-Based Tiled MatMul on KV260 ([arXiv:2503.16731v3](https://arxiv.org/abs/2503.16731)) | Yes ([GitHub](https://github.com/Richielee630/TMMA))                           | DistilBERT              | No                                  | AMD KV260 (Zynq UltraScale+ XCK26 MPSoC) | ~66M      | 71,050 | No              | HLS (Vitis)                          | Yes         | Yes                | Accelerates QKV GEMM ops only; HLS in C++; integrated into quantized DistilBERT with PYNQ                                            |
|  3 | 1a    | TerEffic (https://arxiv.org/abs/2502.16473)                                               | No                                                                             | MatMul-Free LM          | Yes (multi-FPGA or HBM)             | AMD U280 (16nm, 42MB SRAM, 8GB HBM)      | 370M–2.7B | 781k   | No              | RTL (Vivado)                         | No          | Yes                | Two variants: fully on-chip (multi-FPGA) and HBM-assisted; ternary quantized; very efficient                                         |
|  4 | 1a    | MEADOW (https://arxiv.org/abs/2503.11663)                                                 | No                                                                             | OPT-125M                | No                                  | Xilinx ZCU102 (UltraScale+)              | 125M      | 150k   | No              | RTL (Vivado + pipeline)              | Yes         | Yes                | Uses TPHS dataflow + weight packing; evaluated on OPT-125M and OPT-1.3B; 40% end-to-end latency gain                                 |
|  5 | 1a    | FlightLLM (https://arxiv.org/abs/2401.03868)                                              | No (artifact: https://zenodo.org/doi/10.5281/zenodo.10422477)                  | LLaMA2-7B, OPT-6.7B     | Yes (HBM or hybrid HBM+DDR)         | Xilinx Alveo U280; Versal VHK158         | 6.7B–7B   | 574k+  | No              | RTL (Vivado), full flow              | No          | Yes                | Full end-to-end inference; sparse DSP chains; on-chip decode; adaptive compilation; 6× energy efficiency vs V100S.                   |
|  6 | 1a    | SECDA-LLM (https://arxiv.org/abs/2408.00462)                                              | No                                                                             | TinyLlama               | No                                  | PYNQ-Z1 (Xilinx Zynq Z020)               | 1.1B      | ?      | No              | HLS (SystemC via SECDA)              | Yes         | Yes                | Custom MatMul accelerator using BFP quantization; 11× speedup vs ARM NEON CPU; tightly integrated with llama.cpp framework.          |
|  7 | 1a    | MatMul-free LM (https://arxiv.org/abs/2406.02528)                                         | Promised, not delivered (https://github.com/ridgerchu/matmulfreellm/issues/10) | Custom (MatMul-free LM) | No (runs at 13W, no HBM)            | Intel Stratix 10 (D5005 PAC)             | 370M–2.7B | ?      | No              | RTL (SystemVerilog)                  | No          | Yes                | Fully MatMul-free (ternary + elementwise ops); FPGA runs at 60 MHz; 62 tok/s @370M; ~10× lower memory use; custom assembler + ISA    |
|  8 | 1a    | LoopLynx (https://arxiv.org/abs/2504.09561)                                               | Yes (https://github.com/zjnyly/LoopLynx)                                       | GPT-2                   | Yes (multi-FPGA with HBM)           | AMD Alveo U50 (x2), U280                 | 345M      | 624K   | No              | HLS (Vitis), hybrid spatial-temporal | Yes         | Yes                | Hybrid spatial-temporal; 2.52× speedup vs A100 in decode; 4-node setup: 392 tok/s; scalable; task-level and intra-kernel pipelining. |
|  9 | 1a    | MASE https://arxiv.org/abs/2307.15517v2                                                   | Yes (https://github.com/deepwok/mase)                                          | GPT-2                   | No                                  | Xilinx Alveo U250                        | 345M      | ?      | No              | RTL (SystemVerilog)                  | Yes         | Yes                | Focuses on speculative execution for decode stage; achieves 2× throughput vs baseline; supports KV cache and GPT-style LLMs.         |
| 10 | 1a    | On-Device Qwen2.5 https://arxiv.org/abs/2504.17376                                        | No                                                                             | Qwen2.5-0.5B            | No                                  | AMD KV260 (Zynq UltraScale+ XCK26 MPSoC) | 0.5B      | 96,553 | No              | HLS (Vitis), pipelined MAC           | Yes         | Yes                | Uses AWQ (INT4) with hybrid CPU+FPGA execution; 5.1 tok/s; 55.1% model compression rate                                              |
| 11 | 1a    | HLSTransform https://arxiv.org/abs/2405.00738                                             | Yes (https://github.com/HLSTransform/submission)                               | LLaMA 2                 | No                                  | Xilinx Virtex UltraScale+ VU9P           | 110M      | ?      | No              | HLS (Vitis)                          | Yes         | Yes                | Quantized (INT8, Q8_0); achieves 57 tok/s; 12.75× less energy/token vs CPU; open-source end-to-end flow.                             |
| 12 | 1a    | Spatial acceleration https://arxiv.org/abs/2312.15159                                     | Yes (https://github.com/cornell-zhang/allo/tree/main/examples)                 | BERT, GPT-2             | No                                  | AMD Alveo U280                           | 110M–355M | 1.3M   | No              | HLS (Vitis), spatial dataflow        | Yes         | Yes                | Analytical model + open HLS kernels; FPGA outperforms A100 in GPT2 decode; 13.4× faster than prior BERT accelerators on FPGA.        |
| 13 | 1a    | AccLLM https://arxiv.org/abs/2505.03745                                                   | No                                                                             | LLaMA-7B, Falcon-7B     | Yes (multi-FPGA setup)              | AMD Alveo U280 (×4), Xilinx VCU118       | 7B        | 3.4M   | No              | RTL (Verilog/SystemVerilog)          | No          | Yes                | End-to-end deployment; hybrid offloading; sparse matmul + custom interconnect; 2.6× faster than V100 at 3.2× lower energy.           |
| 14 | 1b    | HLS with MLIR and CIRCT https://capra.cs.cornell.edu/latte22/paper/2.pdf                  | Yes (https://github.com/mikeurbach/hot-chips-2022-pytorch-circt-hls-demo)      | No                      | N/A                                 | N/A                                      | N/A       | N/A    | Yes             | RTL (SystemVerilog)                  | No          | No                 | LLM not specifically tageted, PyTorch, in which there are many open source LLMs available                                            |
