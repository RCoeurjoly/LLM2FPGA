* Draft 4
** 1c. Selected Route and Next Steps

This deliverable summarizes the conclusions from the survey (1a) and the compatibility analysis (1b), presents the selected route, and defines fallback options.

*** Summary of findings from 1a and 1b

The survey (1a) identified 14 relevant papers and public repositories related to LLM inference on FPGAs. The main observations are:

- All HLS-based candidates (1, 2, 6, 8, 10, 11 and 12) rely on the proprietary toolchain Vitis HLS.
- An open-source alternative for HLS (Panda Bambu)  exists, but compatibility with existing Vitis HLS code (especially pragmas and headers) is limited and would require substantial manual rework. See Panda bambu section in TMMA analysis in 1b deliverable.
- Only one work (15) demonstrates a full PyTorch-to-FPGA pipeline for an LLM using MLIR-based tooling. Route 14 demostrated the same approach, but with smaller PyTorch input. However, its reference implementation is no longer publicly available, preventing reuse.
- Thanks to NLNet publicity, I got contacted by someone who pointed me to the biggest open source FPGA and its cheapest available board: Xilinx XC7K480T and the board is YPCB-00338-1P1. This FPGA is supported by https://github.com/openXC7/nextpnr-xilinx, and it has approx 5x more LUTs than what I considered to be the biggest previously: Lattice ECP5-85 has 85K LUTs vs Xilinx XC7K480T which has 480k.
- They started this project: https://nlnet.nl/project/PCIe-DMA-DDR3-accelerator/ which got accepted for NLNet funding. This project has a lot of synergies with LLM2FPGA, which we discussed in several calls. Their approach is more bottom up (from PCIe interfaces up to AI accelerators), while mine is top down (from LLM to FPGA). They recommended me to use the hard IP block from Xilinx to make PCIe, if I get to that point before they have finished work on the open source PCIe interface.

These results indicate that most existing approaches are either proprietary, partially closed, or incompatible with a fully open-source FPGA flow.

*** Selected route

Based on the survey and compatibility analysis, the selected route for LLM2FPGA is:

#+begin_example
PyTorch -> Torch-MLIR -> CIRCT -> Verilog -> Yosys
#+end_example

This route is chosen because:

- It is fully open-source and does not depend on proprietary HLS.
- Torch-MLIR provides a maintained and extensible mechanism to lower PyTorch models into MLIR.
- CIRCT enables lowering from MLIR into synthesizable Verilog using hardware-oriented dialects.
- The approach has precedent in published work (*HLSfromPyTorch* (14) and *StreamTensor* (15)), demonstrating technical feasibility.
- The toolchain allows incremental development: unsupported operators can be rewritten, simplified, or decomposed as needed.
- Torch-MLIR and CIRCT are activelly developed (as of December 2025).

*** Risks
- Some PyTorch operators are not supported by Torch-MLIR.
- Lowering an LLM crashes Torch-MLIR or CIRCT.
   
*** Next steps

- Lower a small open-source LLM (TinyStories-2.8M) from PyTorch to Torch-MLIR.
- Identify unsupported operators and document required rewrites.
- Lower the model through CIRCT to Verilog.
- Run Yosys synthesis to assess feasibility and resource usage.
- Document the full pipeline or, if blocked, the failure mode.

* Draft 3
** 1c. Selected route and next steps
This deliverable summarizes the conclusions from the survey (1a) and the compatibility analysis (1b), and presents the selected route, why it is chosen, and the fallback plan.
*** Summary of findings from 1a and 1b
The survey (1a) identified 14 relevant papers and public repositories related to LLM inference on FPGAs. The key observations are:
- All HLS-based candidates (1, 2, 6, 8, 10, 11 and 12) use Vitis HLS, which is proprietary. Panda Bambu seems to be the most promissing open source alternative, but compatibility with Vitis HLS code, specially its headers and pragmas is limited, so manual rework is needed.

* Draft 2.1
** 1c. Results, Selected Route, and Next Steps

This section summarizes the conclusions from the survey (1a) and the compatibility
analysis (1b), and presents the selected kernel, the rationale behind its choice, and
the fallback plan.

*** Summary of Findings from 1a and 1b

The survey conducted in 1a identified more than ten relevant papers and public
repositories related to LLM inference on FPGAs. The key observations were:

- Many HDL-based candidates rely on vendor-specific SystemVerilog or VHDL
  constructs not supported by Yosys or yosys-slang.
- Many HLS-based candidates require proprietary tools such as Vivado HLS or
  Catapult HLS, making them unsuitable for an end-to-end open-source flow.
- Some projects achieve partial success but depend on external hardware, closed
  tooling, or undocumented transformations.
- The only complete PyTorch→FPGA pipeline found in the literature is in the
  *StreamTensor* paper, which demonstrates the feasibility of a full lowering
  stack from PyTorch to FPGA using MLIR and CIRCT. However, the corresponding
  repository has been made private, preventing reproducibility.
- Several candidates exhibited incompatibility during the automated Yosys
  elaboration checks performed in 1b.

These results strongly suggest that open-source viability requires a toolchain that
natively supports both machine-learning IR and hardware IR, without dependence on
vendor tools.

*** Selection Criteria

The following criteria were used to select the kernel (pipeline) for LLM2FPGA:

1. Must support a full end-to-end path: PyTorch → IR → hardware.
2. Must be fully open-source, with no proprietary toolchain dependencies.
3. Must produce Verilog accepted by Yosys for open-source synthesis.
4. Must have active upstream development and long-term maintainability.
5. Should be validated in published literature, even if not fully open-source.
6. Should allow incremental extension (adding missing operators, rewriting models).
7. Should be compatible with quantization and weight-format transformations.

Only one candidate satisfies all these criteria.

*** Selected Route: PyTorch → Torch-MLIR → CIRCT → Verilog → Yosys

After evaluating all surveyed candidates, the selected route is:

#+begin_example
PyTorch → Torch-MLIR → CIRCT → ExportVerilog → Yosys
#+end_example

This route is chosen for the following reasons:

- Torch-MLIR provides the most robust open-source mechanism for exporting PyTorch
  models to MLIR. It is actively maintained and used in multiple research projects.
- CIRCT provides hardware-oriented MLIR dialects (HW, Handshake) and an
  ExportVerilog backend, making it uniquely suited for lowering models into RTL.
- Both projects have active communities with frequent commits and ongoing
  development, increasing confidence in long-term viability.
- The approach has appeared in two separate peer-reviewed papers
  (*HLSfromPyTorch* and *StreamTensor*), demonstrating that the MLIR→hardware
  path is feasible in practice.
- Torch-MLIR and CIRCT allow the introduction of custom passes, operator
  rewrites, tiling, and quantization, all of which are essential for scaling from
  a ~3M-parameter model to a 7B-parameter target in later phases.
- This pipeline is the only one that allows a fully reproducible, open-source
  approach that does not depend on proprietary HLS or vendor-specific HDL
  features.

Therefore, the selected kernel for LLM2FPGA is:

#+begin_example
PyTorch → Torch-MLIR → CIRCT → Verilog
#+end_example

followed by open-source synthesis and resource estimation via Yosys.

*** Relation to Published Work

Two papers directly support the feasibility of this route:

- *HLSfromPyTorch to SystemVerilog with MLIR and CIRCT*  
  Demonstrates lowering from PyTorch to synthesizable hardware using MLIR.
- *StreamTensor*  
  Demonstrates an end-to-end LLM pipeline from PyTorch to FPGA using MLIR
  and CIRCT. Although the source repository was recently made private, the
  methodology is public and aligns with the proposed pipeline.

The existence of these two independent demonstrations provides strong evidence
that the selected approach is viable, reproducible, and extensible.

*** Risks and Expected Challenges

- Torch-MLIR operator coverage for some LLM layers may be incomplete.
- CIRCT may lack direct lowering paths for certain composite operations.
- Performance and resource constraints may require aggressive quantization,
  pruning, or operator rewriting.
- Additional preprocessing may be necessary to convert attention blocks into
  MLIR patterns that CIRCT can lower to hardware.

These risks are mitigated by defining clear fallback strategies.

*** Fallback Plan

1. **Fallback A: Smaller model + operator rewriting**  
   If the chosen model cannot be lowered due to unsupported operators, the
   architecture can be simplified, or unsupported ops can be decomposed into
   primitive MLIR operations.

2. **Fallback B: ONNX route**  
   If Torch-MLIR fails for structural reasons:  
   PyTorch → ONNX → ONNX-MLIR → CIRCT → Verilog.

3. **Fallback C: Manual minimal transformer kernel**  
   If MLIR lowering encounters a fundamental blocker, a minimal transformer
   block will be written in Verilog as a reference HDL kernel, ensuring that the
   project still produces an open-source hardware baseline.

These fallbacks ensure that the project delivers meaningful outputs even in the
presence of toolchain or operator-coverage limitations.

*** Next Steps

- Begin the PyTorch → Torch-MLIR lowering of the smallest LLM candidate
  (TinyStories-2.8M).
- Identify unsupported operators and implement rewrites as necessary.
- Lower the transformed model through CIRCT to Verilog.
- Perform Yosys synthesis to estimate feasibility on target FPGA devices.
- Document the full pipeline or, if blocked, the exact failure mode and
  underlying technical reasons.
* Draft 2
** 1c. Results, Selected Route, and Next Steps

This section summarizes the conclusions from the survey (1a) and the compatibility analysis (1b), and presents the selected path, the rationale behind its choice, and the fallback plan.

*** Summary of Findings from 1a and 1b

The survey conducted in 1a identified more than ten relevant papers and public repositories related to LLM inference on FPGAs. The key observations were:

- Many HLS-based candidates require proprietary tools such as Vivado HLS (Vitis HLS), making them unsuitable for an end-to-end open-source flow.
- The only complete PyTorch→FPGA pipeline found in the literature is in the
  *StreamTensor* paper, which demonstrates the feasibility of a full lowering
  stack from PyTorch to FPGA using MLIR and CIRCT. However, the corresponding
  repository has been made private, preventing reproducibility.

These results strongly suggest that open-source viability requires a toolchain that
natively supports both machine-learning IR and hardware IR, without dependence on
vendor tools.

*** Selection Criteria

The following criteria were used to select the kernel (pipeline) for LLM2FPGA:

1. Must support a full end-to-end path: PyTorch → IR → hardware.
2. Must be fully open-source, with no proprietary toolchain dependencies.
3. Must produce Verilog accepted by Yosys for open-source synthesis.
4. Must have active upstream development and long-term maintainability.
5. Should be validated in published literature, even if not fully open-source.
6. Should allow incremental extension (adding missing operators, rewriting models).
7. Should be compatible with quantization and weight-format transformations.

Only one candidate satisfies all these criteria.

*** Selected Route: PyTorch → Torch-MLIR → CIRCT → Verilog → Yosys

After evaluating all surveyed candidates, the selected route is:

#+begin_example
PyTorch → Torch-MLIR → CIRCT → ExportVerilog → Yosys
#+end_example

This route is chosen for the following reasons:

- Torch-MLIR provides the most robust open-source mechanism for exporting PyTorch
  models to MLIR. It is actively maintained and used in multiple research projects.
- CIRCT provides hardware-oriented MLIR dialects (HW, Handshake) and an
  ExportVerilog backend, making it uniquely suited for lowering models into RTL.
- Both projects have active communities with frequent commits and ongoing
  development, increasing confidence in long-term viability.
- The approach has appeared in two separate peer-reviewed papers
  (*HLSfromPyTorch* and *StreamTensor*), demonstrating that the MLIR→hardware
  path is feasible in practice.
- Torch-MLIR and CIRCT allow the introduction of custom passes, operator
  rewrites, tiling, and quantization, all of which are essential for scaling from
  a ~3M-parameter model to a 7B-parameter target in later phases.
- This pipeline is the only one that allows a fully reproducible, open-source
  approach that does not depend on proprietary HLS or vendor-specific HDL
  features.

Therefore, the selected kernel for LLM2FPGA is:

#+begin_example
PyTorch → Torch-MLIR → CIRCT → Verilog
#+end_example

followed by open-source synthesis and resource estimation via Yosys.

*** Relation to Published Work

Two papers directly support the feasibility of this route:

- *HLSfromPyTorch to SystemVerilog with MLIR and CIRCT*  
  Demonstrates lowering from PyTorch to synthesizable hardware using MLIR.
- *StreamTensor*  
  Demonstrates an end-to-end LLM pipeline from PyTorch to FPGA using MLIR
  and CIRCT. Although the source repository was recently made private, the
  methodology is public and aligns with the proposed pipeline.

The existence of these two independent demonstrations provides strong evidence
that the selected approach is viable, reproducible, and extensible.

*** Risks and Expected Challenges

- Torch-MLIR operator coverage for some LLM layers may be incomplete.
- CIRCT may lack direct lowering paths for certain composite operations.
- Performance and resource constraints may require aggressive quantization,
  pruning, or operator rewriting.
- Additional preprocessing may be necessary to convert attention blocks into
  MLIR patterns that CIRCT can lower to hardware.

These risks are mitigated by defining clear fallback strategies.

*** Fallback Plan

1. **Fallback A: Smaller model + operator rewriting**  
   If the chosen model cannot be lowered due to unsupported operators, the
   architecture can be simplified, or unsupported ops can be decomposed into
   primitive MLIR operations.

2. **Fallback B: ONNX route**  
   If Torch-MLIR fails for structural reasons:  
   PyTorch → ONNX → ONNX-MLIR → CIRCT → Verilog.

https://github.com/onnx/onnx-mlir

3. **Fallback C: Manual minimal transformer kernel**  
   If MLIR lowering encounters a fundamental blocker, a minimal transformer
   block will be written in Verilog as a reference HDL kernel, ensuring that the
   project still produces an open-source hardware baseline.

These fallbacks ensure that the project delivers meaningful outputs even in the
presence of toolchain or operator-coverage limitations.

*** Next Steps

- Begin the PyTorch → Torch-MLIR lowering of the smallest LLM candidate
  (TinyStories-2.8M).
- Identify unsupported operators and implement rewrites as necessary.
- Lower the transformed model through CIRCT to Verilog.
- Perform Yosys synthesis to estimate feasibility on target FPGA devices.
- Document the full pipeline or, if blocked, the exact failure mode and
  underlying technical reasons.
* Draft 1
PyTorch + Torch-MLIR + CIRCT route is the most promising.

It has appeared in two papers: HLSfromPyTorch to System Verilog with MLIR and CIRCT and StreamTensor.

In StreamTensor the whole pipeline from LLM in pytorch to FPGA is demostrated. Unfortunately the git reepo was made private recently, so no access to the source code.

Although Torch-MLIR and CIRCT are not part of the official LLVM release, they have a lot of commits and maintainers and a vibrant community. This gives me confidence that we can rely on them long term.

Torch-MLIR: This project is participating in the LLVM Incubator process: as such, it is not part of any official LLVM release

CIRCT: The CIRCT project is an (experimental!) effort looking to apply MLIR and the LLVM development methodology to the domain of hardware design tools
