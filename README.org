LLM2FPGA is in work in progress.
* Task 1: Survey & candidate selection
State of the art review. Although some candidate projects have been identified, to have the big picture of what has been achieved so far, we need to understand 10 to 15 papers/projects, and what can be reused, even finding synergies between papers/projects.

We are lucky that the keywords LLM and FPGA usually give highly relevant results.
** List of candidate projects identified so far:
arXiv:2307.15517: Fast Prototyping Next-Generation Accelerators for New ML Models using MASE: ML Accelerator System Exploration
arXiv:2504.16266: TeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefilling and Decoding on Edge FPGAs
 arXiv:2503.16731: FPGA-Based Tiled Matrix Multiplication Accelerator for Transformer Self-Attention
 arXiv:2502.16473: TerEffic: Highly Efficient Ternary LLM Inference on FPGA
 arXiv:2503.11663: MEADOW: Memory-efficient Dataflow and Data Packing for Low Power Edge LLMs
 arXiv:2401.03868: FlightLLM: Complete Mapping Flow on FPGAs
 arXiv:2408.00462: Designing Efficient LLM Accelerators for Edge Devices
 arXiv:2406.02528: Scalable MatMul-free Language Modeling 
 arXiv:2504.09561: LoopLynx: A Scalable Dataflow Architecture for Efficient LLM Inference
 arXiv:2504.17376: On-Device Qwen2.5: Efficient LLM Inference with Model Compression and Hardware Acceleration
 arXiv:2405.00738: HLSTransform: Energy-Efficient Llama 2 Inference on FPGAs Via High-Level Synthesis
 arXiv:2312.15159: Understanding the Potential of FPGA-Based Spatial Acceleration for LLM Inference
 arXiv:2505.03745: AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design


